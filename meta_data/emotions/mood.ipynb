{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the audio classification pipeline\n",
        "classifier = pipeline(\"audio-classification\", model=\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")\n",
        "\n",
        "# Path to your folder containing audio files\n",
        "audio_folder = \"/content/drive/MyDrive/audio_segments/Lipstick (feat. Robbie Rise)\"\n",
        "# List to store classification results\n",
        "results_metadata = []\n",
        "\n",
        "# Iterate over all WAV files in the folder\n",
        "for audio_file in os.listdir(audio_folder):\n",
        "    if audio_file.endswith(\".wav\"):  # Check for WAV files\n",
        "        audio_path = os.path.join(audio_folder, audio_file)\n",
        "        try:\n",
        "            # Perform classification\n",
        "            result = classifier(audio_path)\n",
        "\n",
        "            # Select the prediction with the highest score\n",
        "            highest_score_prediction = max(result, key=lambda x: x[\"score\"])\n",
        "\n",
        "            # Append the result as metadata\n",
        "            results_metadata.append({\n",
        "                \"file_name\": audio_file,\n",
        "                \"highest_score_prediction\": highest_score_prediction\n",
        "            })\n",
        "            print(f\"Processed {audio_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_file}: {e}\")\n",
        "\n",
        "# Path to save the metadata JSON file\n",
        "output_json_path = \"/content/Lipstick (feat. Robbie Rise)_mood_metadata.json\"\n",
        "\n",
        "# Save results to JSON file\n",
        "with open(output_json_path, \"w\") as json_file:\n",
        "    json.dump(results_metadata, json_file, indent=4)\n",
        "\n",
        "print(f\"Metadata saved to {output_json_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDLL_LEW1_JC",
        "outputId": "b2b6e27f-a2c8-4666-c585-4a274a655d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:306: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.output.bias', 'classifier.output.weight']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Lipstick (feat. Robbie Rise)_segment_1.wav\n",
            "Processed Lipstick (feat. Robbie Rise)_segment_2.wav\n",
            "Processed Lipstick (feat. Robbie Rise)_segment_3.wav\n",
            "Processed Lipstick (feat. Robbie Rise)_segment_4.wav\n",
            "Processed Lipstick (feat. Robbie Rise)_segment_5.wav\n",
            "Processed Lipstick (feat. Robbie Rise)_segment_6.wav\n",
            "Processed Lipstick (feat. Robbie Rise)_segment_7.wav\n",
            "Processed Lipstick (feat. Robbie Rise)_segment_8.wav\n",
            "Metadata saved to /content/Lipstick (feat. Robbie Rise)_mood_metadata.json\n"
          ]
        }
      ]
    }
  ]
}