{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Genre**"
      ],
      "metadata": {
        "id": "wyeBqrZxwlsO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GON9vnsqf3yO",
        "outputId": "358cb188-00df-4945-85d6-a06256957256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: Rhythm Machine_segment_2.wav\n",
            "Predicted genre: Electronic (Confidence: 74.95%)\n",
            "Processing: Rhythm Machine_segment_4.wav\n",
            "Predicted genre: International (Confidence: 64.78%)\n",
            "Processing: Rhythm Machine_segment_5.wav\n",
            "Predicted genre: Electronic (Confidence: 73.78%)\n",
            "Processing: Rhythm Machine_segment_6.wav\n",
            "Predicted genre: Electronic (Confidence: 67.87%)\n",
            "Processing: Rhythm Machine_segment_1.wav\n",
            "Predicted genre: Electronic (Confidence: 73.83%)\n",
            "Processing: Rhythm Machine_segment_3.wav\n",
            "Predicted genre: Electronic (Confidence: 72.13%)\n",
            "Processing: Rhythm Machine_segment_7.wav\n",
            "Predicted genre: Experimental (Confidence: 34.56%)\n",
            "Processed 7 files. Results saved to /content/rhythm/genre_predictions.json.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "import librosa\n",
        "import torch\n",
        "\n",
        "# Genre mapping\n",
        "genre_mapping = {\n",
        "    0: \"Electronic\",\n",
        "    1: \"Rock\",\n",
        "    2: \"Punk\",\n",
        "    3: \"Experimental\",\n",
        "    4: \"Hip-Hop\",\n",
        "    5: \"Folk\",\n",
        "    6: \"Chiptune / Glitch\",\n",
        "    7: \"Instrumental\",\n",
        "    8: \"Pop\",\n",
        "    9: \"International\",\n",
        "}\n",
        "\n",
        "# Load model and feature extractor using Facebook Wav2vec2 and gastondault Music-classifier\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"gastonduault/music-classifier\")\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large\")\n",
        "\n",
        "# Function for preprocessing audio\n",
        "def preprocess_audio(audio_path):\n",
        "    try:\n",
        "        audio_array, sampling_rate = librosa.load(audio_path, sr=16000)\n",
        "        return feature_extractor(audio_array, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function for predicting genre\n",
        "def predict_genre(audio_path):\n",
        "    inputs = preprocess_audio(audio_path)\n",
        "    if inputs is None:\n",
        "        return None\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        predicted_class = torch.argmax(logits, dim=-1).item()\n",
        "        confidence = torch.softmax(logits, dim=-1)[0, predicted_class].item()\n",
        "    return genre_mapping[predicted_class], confidence\n",
        "\n",
        "# Directory of audio samples\n",
        "audio_directory = \"/content/rhythm\"  # Replace with your directory path\n",
        "output_results = []\n",
        "\n",
        "# Iterate over all .wav files in the directory\n",
        "for filename in os.listdir(audio_directory):\n",
        "    if filename.endswith(\".wav\"):\n",
        "        audio_path = os.path.join(audio_directory, filename)\n",
        "        print(f\"Processing: {filename}\")\n",
        "        result = predict_genre(audio_path)\n",
        "        if result:\n",
        "            genre, confidence = result\n",
        "            output_results.append({\n",
        "                \"filename\": filename,\n",
        "                \"genre\": genre,\n",
        "                \"confidence\": f\"{confidence:.2%}\"\n",
        "            })\n",
        "            print(f\"Predicted genre: {genre} (Confidence: {confidence:.2%})\")\n",
        "\n",
        "# Save results to a JSON file\n",
        "output_file = os.path.join(audio_directory, \"genre_predictions.json\")\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(output_results, f, indent=4)\n",
        "\n",
        "print(f\"Processed {len(output_results)} files. Results saved to {output_file}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Lyrics**"
      ],
      "metadata": {
        "id": "k0kxH2QTwkfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBuxoR2Awxoc",
        "outputId": "8663be3d-d121-4f8b-99e8-944ff549a166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkYg5FF6w3c0",
        "outputId": "2f2eaa1e-761d-4cb1-c0c2-19ece7b610b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803320 sha256=e7e92652e059eb21935914d14a9491edf77794cabbc7c1596458641621f42494\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load the Whisper model (use a model like \"base\" or larger for more accuracy)\n",
        "model = whisper.load_model(\"medium\")\n",
        "\n",
        "# Directory containing audio files\n",
        "audio_dir = \"/content/rhythm\"\n",
        "\n",
        "def transcribe_music(audio_path):\n",
        "    # Transcribe the audio file\n",
        "    result = model.transcribe(audio_path)\n",
        "\n",
        "    # Check if there is any transcribed text\n",
        "    if result[\"text\"].strip() == \"\":\n",
        "        return \"Lyrics are not provided.\"\n",
        "    else:\n",
        "        return result[\"text\"]\n",
        "\n",
        "def transcribe_all_audio_in_directory(directory_path):\n",
        "    # Iterate through all files in the directory\n",
        "    for filename in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "        # Check if the file is an audio file (optional: check for file extension)\n",
        "        if file_path.endswith((\".mp3\", \".wav\", \".flac\", \".ogg\")):\n",
        "            print(f\"Transcribing {filename}...\")\n",
        "            transcription = transcribe_music(file_path)\n",
        "            print(f\"Transcription for {filename}:\")\n",
        "            print(transcription)\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "# Apply the transcription process to all audio files in the directory\n",
        "transcribe_all_audio_in_directory(audio_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxK5qQ3Sw_ub",
        "outputId": "29b01a3e-1f41-480d-e8d1-874f18d879ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:18<00:00, 84.2MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing Rhythm Machine_segment_2.wav...\n",
            "Transcription for Rhythm Machine_segment_2.wav:\n",
            " A love addiction, my intuition A real sensation, an activation You have a mission, cause my ignition A heart admission, intoxication\n",
            "--------------------------------------------------\n",
            "Transcribing Rhythm Machine_segment_4.wav...\n",
            "Transcription for Rhythm Machine_segment_4.wav:\n",
            " You light my fire, my desire, a real feeling, one with meaning You have permission, push my ignition, disarm the system, caught in the rhythm The rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the rhythm, the\n",
            "--------------------------------------------------\n",
            "Transcribing Rhythm Machine_segment_5.wav...\n",
            "Transcription for Rhythm Machine_segment_5.wav:\n",
            " 괜 France próxim weakenMusic RHYTHM MACHINE Music\n",
            "--------------------------------------------------\n",
            "Transcribing Rhythm Machine_segment_6.wav...\n",
            "Transcription for Rhythm Machine_segment_6.wav:\n",
            " ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប ប �\n",
            "--------------------------------------------------\n",
            "Transcribing Rhythm Machine_segment_1.wav...\n",
            "Transcription for Rhythm Machine_segment_1.wav:\n",
            " Rhythm Machine A love addiction My intumation A real sensation An activation You have commission Pushed by ignition A heart omission A real sensation\n",
            "--------------------------------------------------\n",
            "Transcribing Rhythm Machine_segment_3.wav...\n",
            "Transcription for Rhythm Machine_segment_3.wav:\n",
            " I love addiction, my intuition A real sensation, an activation You have permission, push my ignition A hotter mission, intoxication You have permission, push my ignition A hotter mission, intoxication You have permission, push my ignition A hotter mission, intoxication You have permission, push my ignition A hotter mission, intoxication You have permission, push my ignition A hotter mission, intoxication You have permission, push my ignition A hotter mission, intoxication You have permission, push my ignition A hotter mission, intoxication You have permission, push my ignition A hotter mission, intoxication You have permission, push my ignition A hotter mission, intoxication You have permission, push my ignition A hotter mission, intoxication You have permission, push my ignition A hotter mission, intoxication You have permission, push my ignition A hotter mission, intoxication You have permission, push my ignition A hotter mission, intoxication You have permission, push my ignition A hotter mission, intoxication You have permission, push my ignition A hotter mission, intoxication\n",
            "--------------------------------------------------\n",
            "Transcribing Rhythm Machine_segment_7.wav...\n",
            "Transcription for Rhythm Machine_segment_7.wav:\n",
            "Lyrics are not provided.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Mood / Emotions:**"
      ],
      "metadata": {
        "id": "c1kKdjgPx71i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers librosa numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTHwm5zYyAU6",
        "outputId": "ec58818b-b654-4532-a2be-2d4771902951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the audio classification pipeline\n",
        "classifier = pipeline(\"audio-classification\", model=\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")\n",
        "\n",
        "# Path to your folder containing audio files\n",
        "audio_folder = \"/content/rhythm\"\n",
        "\n",
        "# List to store classification results\n",
        "results_metadata = []\n",
        "\n",
        "# Iterate over all WAV files in the folder\n",
        "for audio_file in os.listdir(audio_folder):\n",
        "    if audio_file.endswith(\".wav\"):  # Check for WAV files\n",
        "        audio_path = os.path.join(audio_folder, audio_file)\n",
        "        try:\n",
        "            # Perform classification\n",
        "            result = classifier(audio_path)\n",
        "            # Append the result as metadata\n",
        "            results_metadata.append({\n",
        "                \"file_name\": audio_file,\n",
        "                \"predictions\": result\n",
        "            })\n",
        "            print(f\"Processed {audio_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_file}: {e}\")\n",
        "\n",
        "# Path to save the metadata JSON file\n",
        "output_json_path = \"/content/output/mood_classification_metadata.json\"\n",
        "\n",
        "# Save results to JSON file\n",
        "with open(output_json_path, \"w\") as json_file:\n",
        "    json.dump(results_metadata, json_file, indent=4)\n",
        "\n",
        "print(f\"Metadata saved to {output_json_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80bJEBtpyJbq",
        "outputId": "b28d5cc2-83da-4dfb-df77-cf5b0656f005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.output.bias', 'classifier.output.weight']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Rhythm Machine_segment_2.wav\n",
            "Processed Rhythm Machine_segment_4.wav\n",
            "Processed Rhythm Machine_segment_5.wav\n",
            "Processed Rhythm Machine_segment_6.wav\n",
            "Processed Rhythm Machine_segment_1.wav\n",
            "Processed Rhythm Machine_segment_3.wav\n",
            "Processed Rhythm Machine_segment_7.wav\n",
            "Metadata saved to /content/output/mood_classification_metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Tempo / BPM or Key**\n"
      ],
      "metadata": {
        "id": "WXqbh6zazhdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Musical note mapping based on chroma feature indices\n",
        "note_mapping = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
        "\n",
        "# Function to extract BPM and key\n",
        "def extract_key_bpm(audio_file):\n",
        "    y, sr = librosa.load(audio_file, sr=32000)\n",
        "\n",
        "    # Extract tempo (BPM)\n",
        "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "\n",
        "    # Extract chroma feature and compute the mean of the key\n",
        "    chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
        "\n",
        "    # Find the index of the maximum value in the chroma feature\n",
        "    key_index = chroma.mean(axis=1).argmax()  # Get index of max value across all time frames\n",
        "    key = note_mapping[key_index]  # Map index to corresponding musical note\n",
        "\n",
        "    # Return tempo and key\n",
        "    return tempo, key\n",
        "\n",
        "# List to store extracted features\n",
        "extracted_features = []\n",
        "\n",
        "# Generate and collect metadata for training files\n",
        "train_dir = '/content/rhythm'\n",
        "for file in os.listdir(train_dir):\n",
        "    if file.endswith('.wav'):\n",
        "        file_path = os.path.join(train_dir, file)\n",
        "        bpm, key = extract_key_bpm(file_path)\n",
        "        extracted_features.append([file, bpm, key])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Assuming extracted_features is a list\n",
        "for feature in extracted_features:\n",
        "    print(feature)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY6TbZ4uzyOg",
        "outputId": "8e99299f-62d6-4d53-86f7-49f27cbed2b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Rhythm Machine_segment_2.wav', array([129.31034483]), 'D#']\n",
            "['Rhythm Machine_segment_4.wav', array([129.31034483]), 'D#']\n",
            "['Rhythm Machine_segment_5.wav', array([129.31034483]), 'D#']\n",
            "['Rhythm Machine_segment_6.wav', array([129.31034483]), 'D#']\n",
            "['Rhythm Machine_segment_1.wav', array([129.31034483]), 'D#']\n",
            "['Rhythm Machine_segment_3.wav', array([129.31034483]), 'D#']\n",
            "['Rhythm Machine_segment_7.wav', array([129.31034483]), 'D#']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finalimport os\n",
        "import json\n",
        "import librosa\n",
        "\n",
        "# Musical note mapping based on chroma feature indices\n",
        "note_mapping = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
        "\n",
        "# Function to extract BPM and key\n",
        "def extract_key_bpm(audio_file):\n",
        "    y, sr = librosa.load(audio_file, sr=32000)\n",
        "\n",
        "    # Extract tempo (BPM)\n",
        "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "\n",
        "    # Extract chroma feature and compute the mean of the key\n",
        "    chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
        "\n",
        "    # Find the index of the maximum value in the chroma feature\n",
        "    key_index = chroma.mean(axis=1).argmax()  # Get index of max value across all time frames\n",
        "    key = note_mapping[key_index]  # Map index to corresponding musical note\n",
        "\n",
        "    # Convert tempo to float to ensure JSON serialization\n",
        "    return float(tempo), key\n",
        "\n",
        "# List to store extracted features\n",
        "extracted_features = []\n",
        "\n",
        "# Generate and collect metadata for training files\n",
        "train_dir = '/content/rhythm'  # Replace with your directory path\n",
        "for file in os.listdir(train_dir):\n",
        "    if file.endswith('.wav'):\n",
        "        file_path = os.path.join(train_dir, file)\n",
        "        bpm, key = extract_key_bpm(file_path)\n",
        "        extracted_features.append({\n",
        "            \"filename\": file,\n",
        "            \"tempo\": f\"{bpm:.1f} BPM\",  # Format tempo with \"BPM\"\n",
        "            \"key\": key\n",
        "        })\n",
        "\n",
        "# Save extracted features to a JSON file\n",
        "output_file = os.path.join(train_dir, \"extracted_features.json\")\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(extracted_features, f, indent=4)\n",
        "\n",
        "print(f\"Extracted features saved to {output_file}\")"
      ],
      "metadata": {
        "id": "fW6qJbP_4CfL",
        "outputId": "5b08b50d-e92e-4df5-ac10-40abee16b707",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted features saved to /content/rhythm/extracted_features.json\n"
          ]
        }
      ]
    }
  ]
}